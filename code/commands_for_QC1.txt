
##### QC1 pipeline that contains criteria that was used for MGRB paper (Pinese et al. 2018)

cd ~
cp .bashrc_COPY2019AUG06_PYTHON2_HAIL1___COPY .bashrc
login

contents of .bashrc_COPY2019AUG06_PYTHON2_HAIL1___COPY include:
#export HAIL_HOME=/nvme/emmrat/software_various/hail_2017nov_from_zip/hail  # or wherever you cloned Hail
export HAIL_HOME=/nvme/emmrat/software_various/hail_2017nov_from_git/hail  # or wherever you cloned Hail
export PATH=$PATH:$HAIL_HOME/bin/
#export SPARK_HOME=/nvme/emma_hail_2017may/spark-2.0.2-bin-hadoop2.7  # or wherever you unzipped Spark
#export SPARK_HOME=/nvme/emmrat/software_various/spark/spark-2.4.0-bin-hadoop2.7 # Hail will not work with spark-2.4, need spark-2.0
export SPARK_HOME=/nvme/emmrat/software_various/spark/spark-2.0.2-bin-hadoop2.7
export PYTHONPATH="$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j-0.10.3-src.zip`"
#export PYTHONPATH="$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-*-src.zip"
export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar
export TMPDIR=/nvme/emmrat/tmp_for_hail
export SPARK_LOCAL_DIRS=${TMPDIR}/spark
#export PYSPARK_SUBMIT_ARGS="--driver-memory 500g pyspark-shell"
#export PYSPARK_SUBMIT_ARGS="--driver-memory 50g pyspark-shell --storageLevel MEMORY_AND_DISK"
#export PYSPARK_SUBMIT_ARGS="--driver-memory 450g pyspark-shell --storageLevel MEMORY_AND_DISK"
#export PYSPARK_SUBMIT_ARGS="--driver-memory 450g pyspark-shell --storageLevel MEMORY_AND_DISK --conf spark.network.timeout 10000000 --conf spark.executor.heartbeatInterval=10000000"
#export PYSPARK_SUBMIT_ARGS="--driver-memory 500g pyspark-shell --storageLevel MEMORY_AND_DISK --conf spark.network.timeout=600s --conf spark.executor.heartbeatInterval=600s --conf spark.executor.cores=1 pyspark-shell"
#export PYSPARK_SUBMIT_ARGS="--driver-memory 500g pyspark-shell --storageLevel MEMORY_AND_DISK --conf spark.network.timeout=600s --conf spark.executor.heartbeatInterval=600s --conf spark.executor.cores=54 pyspark-shell"
export PYSPARK_SUBMIT_ARGS="--driver-memory 500g pyspark-shell --storageLevel MEMORY_AND_DISK --conf spark.executor.heartbeatInterval=10000000"
# A guess: When asking for 54 cores or cpus per executor, spark will calculate that we can only have one executor
# When asking for 1 core per executor, spark may launch 56 executors. 
# Those executors have only 1 cpu each, and have to execute the whole JVM (java virtual machine) (the executor is a program that JVM runs).
# Maybe they don't have enough cpu time to reply to heartbeats from the Spark Master, which causes spark (and hail) to crash.
export PATH=/nvme/emma_hail_2017may/spark-2.0.2-bin-hadoop2.7/bin:$PATH



cat /my/cohort/my_cohort_vcf_files.txt # contains one or more vcf files
/my/cohort/files/my_cohort.part1.vcf.bgz
/my/cohort/files/my_cohort.part2.vcf.bgz

python code/calculate_mgrb_qc_pipeline_in_hail_for_our_cohort.py \
	-infiles /my/cohort/my_cohort_vcf_files.txt \
	-outfile /my/cohort/output/my_cohort.InfiniumQCArray_24v1_0_A3.qc1_pipeline.cvs


